# -*- coding: utf-8 -*-
"""cifar10 autoattack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hARkBQpRwNh-iFNBJKLOqKNQEcKk8Qgm
"""

# !pip install git+https://github.com/fra31/auto-attack

# https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/
# Load in relevant libraries, and alias where appropriate
# from autoattack import AutoAttack 
from tqdm import tqdm
import numpy as np
from torchvision import transforms, datasets
from torch.utils.data import Dataset
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
# import torchvision
from torchvision import transforms, datasets
from torch.utils.data import Dataset
from PIL import Image
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader
from cleverhans.torch.attacks.projected_gradient_descent import projected_gradient_descent as pgd
from torch.utils.data.sampler import SubsetRandomSampler
from cifar10_utils import ResNet18, get_CIFAR10
import random
def set_seeds(seed):
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)


# Define relevant variables for the ML task
batch_size = 128
num_classes = 10
learning_rate = 0.1
num_epochs = 2
optparams = {'weight_decay': 0.0005, 'momentum': 0.9}
# Device will determine whether to run the training on GPU or CPU.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


train_dataset, test_dataset = get_CIFAR10(45000)
valid_size= 5000
indices = torch.randperm(len(train_dataset))
train_indices = indices[:len(indices) - valid_size]
valid_indices = indices[len(indices) - valid_size:] if valid_size else None

# Make dataloaders
train_loader = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=batch_size,
                                            sampler=SubsetRandomSampler(train_indices))
valid_loader = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=batch_size,
                                            sampler=SubsetRandomSampler(valid_indices))


test_loader = torch.utils.data.DataLoader(dataset = test_dataset,
                                           batch_size = batch_size,
                                           shuffle = True)

model = ResNet18().to(device)

#Setting the loss function
cost = nn.CrossEntropyLoss()

#Setting the optimizer with the model parameters and learning rate
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, **optparams)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)
#
#this is defined to print how many steps are remaining when training
total_step = len(train_loader)

# for i, (x,y, idx) in enumerate(train_loader):  
#   print(i, x.shape, y.shape, len(idx))
#   break

total_step = len(train_loader)
for epoch in tqdm(range(num_epochs)):
    for i, (images, labels, idx) in enumerate(train_loader):  
        images = images.to(device)
        labels = labels.to(device)
        
        #Forward pass
        outputs = model(images)
        loss = cost(outputs, labels)
        	
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        		
        if (i+1) % 400 == 0:
            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' 
        		           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))
    scheduler.step()
#save model
torch.save(model.state_dict(), './cifarmodel/')

with torch.no_grad():
    correct = 0
    total = 0
    for images, labels, idx in tqdm(test_loader):
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))
