{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zvqXNCFsdfX"
      },
      "source": [
        "# Preambule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "839CHrsNqyim",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/fra31/auto-attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I92xPOj1qbjr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# from autoattack import AutoAttack \n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yn2n0juZnjk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/jonas/Downloads/autoattack.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/Downloads/autoattack.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/Downloads/autoattack.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Load in relevant libraries, and alias where appropriate\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jonas/Downloads/autoattack.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/Downloads/autoattack.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/Downloads/autoattack.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/\n",
        "# Load in relevant libraries, and alias where appropriate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.notebook import tqdm\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "# Define relevant variables for the ML task\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkSy_ZCTX-Fy",
        "outputId": "c9151bfc-4181-4aec-86dd-8b7f910e4f71",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd3T1aDOuPqI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def set_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj7cUloBMRud"
      },
      "source": [
        "# MC dropout & Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "psuAbgSPMToi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title MC LeNet5\n",
        "class Dropout2dLayer(nn.Dropout2d):\n",
        "  def __init__(self, droprate=0.5, inplace=False):\n",
        "    super().__init__(p=droprate, inplace=inplace)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return F.dropout2d(input, self.p, training=True, inplace=self.inplace)\n",
        "\n",
        "class MCLeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        # num_classes = 10\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
        "            Dropout2dLayer(0.2),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
        "            Dropout2dLayer(0.2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(400, 120)\n",
        "        # self.dropout2dlayer1 = Dropout2dLayer(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        # self.dropout2dlayer2 = Dropout2dLayer(0.5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc_head = nn.Linear(84, num_classes)\n",
        "\n",
        "    def embedding(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)    \n",
        "        \n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        # out = self.dropout2dlayer1(out)\n",
        "        out = F.dropout(x, 0.5, training=True, inplace=False)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        # out = self.dropout2dlayer2(out)\n",
        "        out = F.dropout(input, 0.5, training=True, inplace=False)\n",
        "        out = self.relu1(out)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.e1 = self.embedding(x)\n",
        "        out = self.fc_head(self.e1)\n",
        "        return out\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ilDk2jQRleb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Ensemble/initialization\n",
        "def init_weights(init_function, **kwargs):\n",
        "  def _init_weights(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.kaiming_uniform_(module.weight, **kwargs)\n",
        "        if module.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n",
        "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
        "            nn.init.uniform_(module.bias, -bound, bound)\n",
        "\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        init_function(module.weight, **kwargs)\n",
        "        if module.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n",
        "            if fan_in != 0:\n",
        "                bound = 1 / math.sqrt(fan_in)\n",
        "                nn.init.uniform_(module.bias, -bound, bound)\n",
        "  return _init_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5J7J4t8ZsHu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# a=math.sqrt(5)\n",
        "init_list = [nn.init.kaiming_uniform_, nn.init.kaiming_normal_, nn.init.xavier_uniform_, nn.init.xavier_normal_, nn.init.uniform_]\n",
        "initialization1 = init_weights(nn.init.kaiming_uniform_, a=math.sqrt(5))\n",
        "# model.apply(initialization1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C30yoRUc9Ue",
        "outputId": "ee311f16-bcd5-4a81-b855-0ff7a987ef43",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title sampler\n",
        "generator = torch.Generator(device).manual_seed(4599)\n",
        "sampler = torch.utils.data.RandomSampler(train_dataset, generator=generator)\n",
        "dtl = torch.utils.data.DataLoader(train_dataset, batch_size=10, sampler=sampler)\n",
        "for x, y in dtl:\n",
        "  print(x.mean(), y.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWAq1O1zqdJC"
      },
      "source": [
        "# Train Eval 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3wSLJ2-uOpiL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title load mnist\n",
        "#Loading the dataset and preprocessing\n",
        "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
        "                                           train = True,\n",
        "                                           transform = transforms.Compose([\n",
        "                                                  transforms.Resize((32,32)),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
        "                                           download = True)\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
        "                                          train = False,\n",
        "                                          transform = transforms.Compose([\n",
        "                                                  transforms.Resize((32,32)),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
        "                                          download=True)\n",
        "\n",
        "valid_size= 5000\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [len(train_dataset)-valid_size, valid_size])\n",
        "\n",
        "# Make dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=batch_size)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, pin_memory=True)\n",
        "\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "#                                            batch_size = batch_size,\n",
        "#                                            shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "len(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "w0WAOhFjZTTA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title cifar10_handler\n",
        "class CIFAR10_Handler(Dataset):\n",
        "    def __init__(self, X, Y, train=True):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        if train:\n",
        "            self.transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.RandomCrop(32, padding=4),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(\n",
        "                        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            self.transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(\n",
        "                        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.X[index], self.Y[index]\n",
        "        x = Image.fromarray(x)\n",
        "        x = self.transform(x)\n",
        "        return x, y, index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "# Data.py\n",
        "def get_CIFAR10(pool_size):\n",
        "    data_train = datasets.CIFAR10('./data/CIFAR10', train=True, download=True)\n",
        "    data_test = datasets.CIFAR10('./data/CIFAR10', train=False, download=True)\n",
        "    dtrain =  CIFAR10_Handler(data_train.data[:pool_size], torch.LongTensor(data_train.targets)[:pool_size])\n",
        "    dtest = CIFAR10_Handler(data_test.data[:pool_size], torch.LongTensor(data_test.targets)[:pool_size])\n",
        "    return dtrain, dtest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "09a2297ed05e4418a70571ce467a06d9",
            "0656fda51c7345ea9567e55ccd287894",
            "d7fd83c4f42f41be9636356b6e57403e",
            "ee4e4045a51649baae806d2dcc3f88a1",
            "a4a1f5cf2b004536bfcfac2c044b66cc",
            "0d9be0b924294f2e97b72d551d3ee698",
            "58f32065966847688ee268ee804212f1",
            "44c2240be0644591aaae680e02388df0",
            "26f8ba678bd94de790854d597858fa89",
            "b3ce6aebb2e64bcc9908027e4390b279",
            "8fb2a4fc36b14239971379c65232a73d"
          ]
        },
        "id": "N-EjULN8ZX39",
        "outputId": "3a8af2e2-40d9-4763-a621-98c9a014ccb2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title cifar10\n",
        "train_dataset, test_dataset = get_CIFAR10(45000)\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "58YKD6-OOqa4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title LeNet5\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        # num_classes = 10\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(400, 120)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc_head = nn.Linear(84, num_classes)\n",
        "\n",
        "    def embedding(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.e1 = self.embedding(x)\n",
        "        out = self.fc_head(self.e1)\n",
        "        return out\n",
        "      \n",
        "    # def predict(self, x):\n",
        "    #     with torch.no_grad():\n",
        "    #       return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I2d3aAxKS5rr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title resnet model\n",
        "'''ResNet in PyTorch.\n",
        "\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "# src: https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def embedding(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.e1 = self.embedding(x)\n",
        "        out = self.e1.view(self.e1.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N93-xtcQVH0H",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Data Class\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, X_train, Y_train, X_test, Y_test, handler, n_adv_test):\n",
        "        self.X_train = X_train\n",
        "        self.Y_train = Y_train\n",
        "        self.X_test = X_test\n",
        "        self.Y_test = Y_test\n",
        "        self.handler = handler\n",
        "\n",
        "        self.n_pool = len(X_train)\n",
        "        self.n_test = len(X_test)\n",
        "\n",
        "        self.labeled_idxs = np.zeros(self.n_pool, dtype=bool)\n",
        "        # To handle addition of adversarial dataset to labelled pool\n",
        "        self.X_train_extra = torch.Tensor([])\n",
        "        self.Y_train_extra = torch.Tensor([])\n",
        "        # adv test data\n",
        "        self.n_adv_test = n_adv_test\n",
        "        if self.n_adv_test == self.n_test:\n",
        "            self.adv_test_idxs = np.arange(self.n_test)\n",
        "        else:\n",
        "            self.adv_test_idxs = np.random.choice(\n",
        "                np.arange(self.n_test), self.n_adv_test, replace=False)\n",
        "\n",
        "    def initialize_labels(self, num):\n",
        "        # generate initial labeled pool\n",
        "        tmp_idxs = np.arange(self.n_pool)\n",
        "        np.random.shuffle(tmp_idxs)\n",
        "        # print(tmp_idxs[:5], tmp_idxs[-5:])\n",
        "        self.labeled_idxs[tmp_idxs[:num]] = True\n",
        "\n",
        "    def add_extra_data(self, pos_idxs, extra_data):\n",
        "        # print('Y_train_extra', self.Y_train[pos_idxs])\n",
        "        if len(self.X_train_extra) > 0:\n",
        "            self.X_train_extra = torch.vstack([self.X_train_extra, extra_data])\n",
        "            self.Y_train_extra = torch.hstack(\n",
        "                [self.Y_train_extra, self.Y_train[pos_idxs]])\n",
        "        else:\n",
        "            self.X_train_extra = extra_data\n",
        "            self.Y_train_extra = self.Y_train[pos_idxs]\n",
        "        # assert len(self.X_train_extra) == len(self.Y_train_extra)\n",
        "        # print('New Y_train_extra', self.Y_train_extra)\n",
        "\n",
        "    def get_labeled_data(self):\n",
        "        labeled_idxs = np.arange(self.n_pool)[self.labeled_idxs]\n",
        "        if len(self.X_train_extra) > 0:\n",
        "            # print('data.py:44',self.X_train[labeled_idxs].shape, self.X_train_extra.shape)\n",
        "            if len(self.X_train_extra.shape) - len(self.X_train.shape) == -1:\n",
        "                X_train_extra = self.X_train_extra.unsqueeze(1)\n",
        "            elif len(self.X_train_extra.shape) - len(self.X_train.shape) == 1:\n",
        "                X_train_extra = self.X_train_extra.squeeze(1)\n",
        "            else:\n",
        "                X_train_extra = self.X_train_extra\n",
        "            # breakpoint()\n",
        "            if torch.is_tensor(X_train_extra):\n",
        "                self.X_train = torch.tensor(self.X_train)\n",
        "                \n",
        "            X = torch.vstack((self.X_train[labeled_idxs], X_train_extra))\n",
        "            Y = torch.hstack([self.Y_train[labeled_idxs], self.Y_train_extra])\n",
        "        else:\n",
        "            X = self.X_train[labeled_idxs]\n",
        "            Y = self.Y_train[labeled_idxs]\n",
        "        return labeled_idxs, self.handler(X, Y)\n",
        "\n",
        "    def n_labeled(self):\n",
        "        return sum(self.labeled_idxs) + len(self.X_train_extra)\n",
        "\n",
        "    def get_unlabeled_data(self, n_subset=None):\n",
        "        unlabeled_idxs = np.arange(self.n_pool)[~self.labeled_idxs]\n",
        "        np.random.shuffle(unlabeled_idxs)\n",
        "        if n_subset:\n",
        "            unlabeled_idxs = unlabeled_idxs[:n_subset]\n",
        "        X = self.X_train[unlabeled_idxs]\n",
        "        Y = self.Y_train[unlabeled_idxs]\n",
        "        return unlabeled_idxs, self.handler(X, Y)\n",
        "\n",
        "    def get_train_data(self):\n",
        "        return self.labeled_idxs.copy(), self.handler(self.X_train, self.Y_train)\n",
        "\n",
        "    def get_test_data(self):\n",
        "        return self.handler(self.X_test, self.Y_test, train=False)\n",
        "\n",
        "    def get_adv_test_data(self):\n",
        "        return self.handler(self.X_test[self.adv_test_idxs], self.Y_test[self.adv_test_idxs], train=False)\n",
        "\n",
        "    def cal_test_acc(self, preds):\n",
        "        return 100.0 * (self.Y_test == preds).sum().item() / self.n_test\n",
        "\n",
        "    def cal_adv_test_acc(self, preds):\n",
        "        return 100.0 * (self.Y_test[self.adv_test_idxs] == preds).sum().item() / self.n_adv_test\n",
        "\n",
        "\n",
        "def get_xMNIST(x_fn, handler, pool_size, n_adv_test, pref = ''):\n",
        "    # transform_ = transforms.Compose([\n",
        "    #                                               transforms.Resize((32,32)),\n",
        "    #                                               transforms.ToTensor(),\n",
        "    #                                               transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
        "    # transform_ = ToTensor()\n",
        "    raw_train = x_fn(root='./data/'+pref+'MNIST', train=True, download=True, transform=transforms.Compose([\n",
        "                                                      transforms.Resize((32,32)),\n",
        "                                                      transforms.ToTensor(),\n",
        "                                                      transforms.Normalize(mean = (0.1307,), std = (0.3081,))]))\n",
        "    raw_test = x_fn(root='./data/'+pref+'MNIST', train=False, download=True, transform=transforms.Compose([\n",
        "                                                      transforms.Resize((32,32)),\n",
        "                                                      transforms.ToTensor(),\n",
        "                                                      transforms.Normalize(mean = (0.1307,), std = (0.3081,))]))\n",
        "\n",
        "    # raw_train = x_fn(root = './data',\n",
        "    #                                           train = True,\n",
        "    #                                           transform = transforms.Compose([\n",
        "    #                                                   transforms.Resize((32,32)),\n",
        "    #                                                   transforms.ToTensor(),\n",
        "    #                                                   transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
        "    #                                           download = True)\n",
        "\n",
        "    dtl = DataLoader(raw_train, batch_size=len(raw_train))\n",
        "    for X, y in dtl:\n",
        "        X_train = X\n",
        "        Y_train = y\n",
        "\n",
        "    dtl = DataLoader(raw_test, batch_size=len(raw_test))\n",
        "    for X,y in dtl:\n",
        "        X_test = X\n",
        "        Y_test = y\n",
        "    return Data(X_train[:pool_size], Y_train[:pool_size], X_test, Y_test, handler, n_adv_test)\n",
        "\n",
        "\n",
        "def get_MNIST(handler, pool_size, n_adv_test):\n",
        "    return get_xMNIST(datasets.MNIST, handler, pool_size, n_adv_test)\n",
        "\n",
        "class MNIST_Handler(Dataset):\n",
        "    def __init__(self, X, Y, train=True):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        # self.transform= transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.X[index], self.Y[index]\n",
        "        # if not isinstance(x, np.ndarray):\n",
        "        #     x = Image.fromarray(x.numpy(), mode='L')\n",
        "        #     x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "data = get_MNIST(MNIST_Handler, 45000, 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuK2s2ZHXmlm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# data.initialize_labels(60000)\n",
        "# _,trd = data.get_labeled_data()\n",
        "# train_loader = DataLoader(trd, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJa9oiv0rSiP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = LeNet5().to(device)\n",
        "# model = MCLeNet5().to(device)\n",
        "\n",
        "#Setting the loss function\n",
        "cost = nn.CrossEntropyLoss()\n",
        "\n",
        "#Setting the optimizer with the model parameters and learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#this is defined to print how many steps are remaining when training\n",
        "total_step = len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CFbKeRDSTHyv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title resnet train\n",
        "\n",
        "# Define relevant variables for the ML task\n",
        "batch_size = 256\n",
        "num_classes = 10\n",
        "learning_rate = 0.1\n",
        "num_epochs = 100\n",
        "optparams = {'weight_decay': 0.0005, 'momentum': 0.9}\n",
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ResNet18().to(device)\n",
        "\n",
        "#Setting the loss function\n",
        "cost = nn.CrossEntropyLoss()\n",
        "\n",
        "#Setting the optimizer with the model parameters and learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, **optparams)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "#\n",
        "#this is defined to print how many steps are remaining when training\n",
        "total_step = len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9spaXPF46o3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWvW0fvzcOHI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# batch_size = 8\n",
        "# train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "#                                            batch_size = batch_size,\n",
        "#                                            shuffle = True)\n",
        "# for i, (images, labels) in enumerate(train_loader):\n",
        "#   print(images.shape, type(images), images.dtype)\n",
        "#   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bVJUDU5sOfp",
        "outputId": "9e505dfa-27ba-4abf-a0ee-f2a57c33e550",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "total_step = len(train_loader)\n",
        "model.train()  # set train mode\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        #Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = cost(outputs, labels)\n",
        "        \t\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \t\t\n",
        "        if (i+1) % 400 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "        \t\t           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuz1e6K1w5Vi",
        "outputId": "618b3486-54f1-4b77-9255-feb7cbeb2f2a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3k4x78usYuY",
        "outputId": "21b3e8db-3142-4abe-967e-f5961ea04e7c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Clean Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\t "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu_s2XmOgqrb"
      },
      "source": [
        "# Temp Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class ModelWithTemperature(nn.Module):\n",
        "    \"\"\"\n",
        "    A thin decorator, which wraps a model with temperature scaling\n",
        "    model (nn.Module):\n",
        "        A classification neural network\n",
        "        NB: Output of the neural network should be the classification logits,\n",
        "            NOT the softmax (or log softmax)!\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.temperature = temperature = nn.Parameter(torch.ones(1).cuda())\n",
        "\n",
        "    def forward(self, input):\n",
        "        logits = self.model(input)\n",
        "        return self.temperature_scale(logits)\n",
        "\n",
        "    def temperature_scale(self, logits):\n",
        "        \"\"\"\n",
        "        Perform temperature scaling on logits\n",
        "        \"\"\"\n",
        "        # Expand temperature to match the size of logits\n",
        "        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n",
        "        return logits / temperature\n",
        "\n",
        "    def T_scaling(self, logits):\n",
        "        return torch.div(logits, self.temperature)\n",
        "\n",
        "    # This function probably should live outside of this class, but whatever\n",
        "    def set_temperature(self, valid_loader):\n",
        "        \"\"\"\n",
        "        Tune the tempearature of the model (using the validation set).\n",
        "        We're going to set it to optimize NLL.\n",
        "        valid_loader (DataLoader): validation set loader\n",
        "        \"\"\"\n",
        "        self.to(device)\n",
        "        nll_criterion = nn.CrossEntropyLoss().to(device)\n",
        "        ece_criterion = _ECELoss().to(device)\n",
        "\n",
        "        # First: collect all the logits and labels for the validation set\n",
        "        logits_list = []\n",
        "        labels_list = []\n",
        "        temps = []\n",
        "        losses = []\n",
        "\n",
        "        for i, data in enumerate(tqdm(valid_loader, 0)):\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                logits_list.append(self.model(images))\n",
        "                labels_list.append(labels)\n",
        "\n",
        "        # Create tensors\n",
        "        logits_list = torch.cat(logits_list).to(device)\n",
        "        labels_list = torch.cat(labels_list).to(device)\n",
        "\n",
        "\n",
        "        # Calculate NLL and ECE before temperature scaling\n",
        "        before_temperature_nll = nll_criterion(logits_list, labels).item()\n",
        "        before_temperature_ece = ece_criterion(logits_list, labels).item()\n",
        "        print('Before temperature - NLL: %.3f, ECE: %.3f' % (before_temperature_nll, before_temperature_ece))\n",
        "\n",
        "        # Next: optimize the temperature w.r.t. NLL\n",
        "        # Removing strong_wolfe line search results in jump after 50 epochs\n",
        "        optimizer = torch.optim.LBFGS([self.temperature], lr=0.001, max_iter=10000, line_search_fn='strong_wolfe')\n",
        "\n",
        "    \n",
        "        def _eval():\n",
        "            loss = nll_criterion(self.T_scaling(logits_list), labels_list)\n",
        "            loss.backward()\n",
        "            temps.append(self.temperature.item())\n",
        "            losses.append(loss)\n",
        "            return loss\n",
        "\n",
        "\n",
        "        optimizer.step(_eval)\n",
        "        \n",
        "        # Calculate NLL and ECE after temperature scaling\n",
        "        after_temperature_nll = nll_criterion(self.temperature_scale(logits_list), labels).item()\n",
        "        after_temperature_ece = ece_criterion(self.temperature_scale(logits_list), labels).item()\n",
        "        print('Optimal temperature: %.3f' % self.temperature.item())\n",
        "        print('After temperature - NLL: %.3f, ECE: %.3f' % (after_temperature_nll, after_temperature_ece))\n",
        "\n",
        "        plt.subplot(121)\n",
        "        plt.plot(list(range(len(temps))), temps)\n",
        "\n",
        "        plt.subplot(122)\n",
        "        xlosses = [x.detach().cpu() for x in losses]\n",
        "        plt.plot(list(range(len(xlosses))), xlosses)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        return self\n",
        "\n",
        "class _ECELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Calculates the Expected Calibration Error of a model.\n",
        "    (This isn't necessary for temperature scaling, just a cool metric).\n",
        "\n",
        "    The input to this loss is the logits of a model, NOT the softmax scores.\n",
        "\n",
        "    This divides the confidence outputs into equally-sized interval bins.\n",
        "    In each bin, we compute the confidence gap:\n",
        "\n",
        "    bin_gap = | avg_confidence_in_bin - accuracy_in_bin |\n",
        "\n",
        "    We then return a weighted average of the gaps, based on the number\n",
        "    of samples in each bin\n",
        "\n",
        "    See: Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht.\n",
        "    \"Obtaining Well Calibrated Probabilities Using Bayesian Binning.\" AAAI.\n",
        "    2015.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4cIMdvA9gyiA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title https://github.com/gpleiss/temperature_scaling\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    \"\"\"\n",
        "    A thin decorator, which wraps a model with temperature scaling\n",
        "    model (nn.Module):\n",
        "        A classification neural network\n",
        "        NB: Output of the neural network should be the classification logits,\n",
        "            NOT the softmax (or log softmax)!\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
        "\n",
        "    def forward(self, input):\n",
        "        logits = self.model(input)\n",
        "        return self.temperature_scale(logits)\n",
        "\n",
        "    def temperature_scale(self, logits):\n",
        "        \"\"\"\n",
        "        Perform temperature scaling on logits\n",
        "        \"\"\"\n",
        "        # Expand temperature to match the size of logits\n",
        "        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n",
        "        return logits / temperature\n",
        "\n",
        "    # This function probably should live outside of this class, but whatever\n",
        "    def set_temperature(self, valid_loader):\n",
        "        \"\"\"\n",
        "        Tune the tempearature of the model (using the validation set).\n",
        "        We're going to set it to optimize NLL.\n",
        "        valid_loader (DataLoader): validation set loader\n",
        "        \"\"\"\n",
        "        self.to(device)\n",
        "        nll_criterion = nn.CrossEntropyLoss().to(device)\n",
        "        ece_criterion = _ECELoss().to(device)\n",
        "\n",
        "        # First: collect all the logits and labels for the validation set\n",
        "        logits_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for input, label in valid_loader:\n",
        "                input = input.to(device)\n",
        "                logits = self.model(input)\n",
        "                logits_list.append(logits)\n",
        "                labels_list.append(label)\n",
        "            logits = torch.cat(logits_list).to(device)\n",
        "            labels = torch.cat(labels_list).to(device)\n",
        "\n",
        "        # Calculate NLL and ECE before temperature scaling\n",
        "        before_temperature_nll = nll_criterion(logits, labels).item()\n",
        "        before_temperature_ece = ece_criterion(logits, labels).item()\n",
        "        print('Before temperature - NLL: %.3f, ECE: %.3f' % (before_temperature_nll, before_temperature_ece))\n",
        "\n",
        "        # Next: optimize the temperature w.r.t. NLL\n",
        "        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "        def eval():\n",
        "            optimizer.zero_grad()\n",
        "            loss = nll_criterion(self.temperature_scale(logits), labels)\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimizer.step(eval)\n",
        "\n",
        "        # Calculate NLL and ECE after temperature scaling\n",
        "        after_temperature_nll = nll_criterion(self.temperature_scale(logits), labels).item()\n",
        "        after_temperature_ece = ece_criterion(self.temperature_scale(logits), labels).item()\n",
        "        print('Optimal temperature: %.3f' % self.temperature.item())\n",
        "        print('After temperature - NLL: %.3f, ECE: %.3f' % (after_temperature_nll, after_temperature_ece))\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "class _ECELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Calculates the Expected Calibration Error of a model.\n",
        "    (This isn't necessary for temperature scaling, just a cool metric).\n",
        "\n",
        "    The input to this loss is the logits of a model, NOT the softmax scores.\n",
        "\n",
        "    This divides the confidence outputs into equally-sized interval bins.\n",
        "    In each bin, we compute the confidence gap:\n",
        "\n",
        "    bin_gap = | avg_confidence_in_bin - accuracy_in_bin |\n",
        "\n",
        "    We then return a weighted average of the gaps, based on the number\n",
        "    of samples in each bin\n",
        "\n",
        "    See: Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht.\n",
        "    \"Obtaining Well Calibrated Probabilities Using Bayesian Binning.\" AAAI.\n",
        "    2015.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcF7Jobxgs8O",
        "outputId": "07d1986c-b0ce-4020-9ce6-fe4fcfa22cbe",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# orig_model = ... # create an uncalibrated model somehow\n",
        "# valid_loader = ... # Create a DataLoader from the SAME VALIDATION SET used to train orig_model\n",
        "scaled_model = ModelWithTemperature(model)\n",
        "scaled_model.set_temperature(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRUp-u-RmjQE",
        "outputId": "9876bab7-e371-4ae7-8014-47362e32331f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Clean Test scaled model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "scaled_model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = scaled_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\t "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOEVjhJJom9u",
        "outputId": "654ba28b-b73f-4ce3-99e9-33e9de16ce4d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title temp scale\n",
        "nll_criterion = nn.CrossEntropyLoss().to(device)\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "# First: collect all the logits and labels for the validation set\n",
        "logits_list = []\n",
        "labels_list = []\n",
        "with torch.no_grad():\n",
        "    for input, label in test_loader:\n",
        "        input = input.to(device)\n",
        "        logits = model(input)\n",
        "        logits_list.append(logits)\n",
        "        labels_list.append(label)\n",
        "    logits = torch.cat(logits_list).to(device)\n",
        "    labels = torch.cat(labels_list).to(device)\n",
        "\n",
        "# Calculate NLL and ECE before temperature scaling\n",
        "before_temperature_nll = nll_criterion(logits, labels).item()\n",
        "before_temperature_ece = ece_criterion(logits, labels).item()\n",
        "print('Before temperature - NLL: %.3f, ECE: %.3f' % (before_temperature_nll, before_temperature_ece))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azAqn5Lvm-yE",
        "outputId": "c086190a-2e3b-4d99-f0f6-af72c5cd5116",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title temp scale\n",
        "nll_criterion = nn.CrossEntropyLoss().to(device)\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "# First: collect all the logits and labels for the validation set\n",
        "logits_list = []\n",
        "labels_list = []\n",
        "with torch.no_grad():\n",
        "    for input, label in test_loader:\n",
        "        input = input.to(device)\n",
        "        logits = scaled_model(input)\n",
        "        logits_list.append(logits)\n",
        "        labels_list.append(label)\n",
        "    logits = torch.cat(logits_list).to(device)\n",
        "    labels = torch.cat(labels_list).to(device)\n",
        "\n",
        "after_temperature_nll = nll_criterion(scaled_model.temperature_scale(logits), labels).item()\n",
        "after_temperature_ece = ece_criterion(scaled_model.temperature_scale(logits), labels).item()\n",
        "print('Optimal temperature: %.3f' % scaled_model.temperature.item())\n",
        "print('After temperature - NLL: %.3f, ECE: %.3f' % (after_temperature_nll, after_temperature_ece))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YYpzBExIwtKG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title https://github.com/google/uncertainty-baselines/blob/main/baselines/mnist/utils.py\n",
        "# coding=utf-8\n",
        "# Copyright 2022 The Uncertainty Baselines Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Utilities for (Fashion) MNIST.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "\n",
        "def one_hot(a, num_classes):\n",
        "  return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n",
        "\n",
        "\n",
        "def brier_score(y, p):\n",
        "  \"\"\"Compute the Brier score.\n",
        "\n",
        "  Brier Score: see\n",
        "  https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf,\n",
        "  page 363, Example 1\n",
        "\n",
        "  Args:\n",
        "    y: one-hot encoding of the true classes, size (?, num_classes)\n",
        "    p: numpy array, size (?, num_classes)\n",
        "       containing the output predicted probabilities\n",
        "  Returns:\n",
        "    bs: Brier score.\n",
        "  \"\"\"\n",
        "  return np.mean(np.power(p - y, 2))\n",
        "\n",
        "\n",
        "def calibration(y, p_mean, num_bins=10):\n",
        "  \"\"\"Compute the calibration.\n",
        "\n",
        "  References:\n",
        "  https://arxiv.org/abs/1706.04599\n",
        "  https://arxiv.org/abs/1807.00263\n",
        "\n",
        "  Args:\n",
        "    y: one-hot encoding of the true classes, size (?, num_classes)\n",
        "    p_mean: numpy array, size (?, num_classes)\n",
        "           containing the mean output predicted probabilities\n",
        "    num_bins: number of bins\n",
        "\n",
        "  Returns:\n",
        "    ece: Expected Calibration Error\n",
        "    mce: Maximum Calibration Error\n",
        "  \"\"\"\n",
        "  # Compute for every test sample x, the predicted class.\n",
        "  class_pred = np.argmax(p_mean, axis=1)\n",
        "  # and the confidence (probability) associated with it.\n",
        "  conf = np.max(p_mean, axis=1)\n",
        "  # Convert y from one-hot encoding to the number of the class\n",
        "  y = np.argmax(y, axis=1)\n",
        "  # Storage\n",
        "  acc_tab = np.zeros(num_bins)  # empirical (true) confidence\n",
        "  mean_conf = np.zeros(num_bins)  # predicted confidence\n",
        "  nb_items_bin = np.zeros(num_bins)  # number of items in the bins\n",
        "  tau_tab = np.linspace(0, 1, num_bins+1)  # confidence bins\n",
        "  for i in np.arange(num_bins):  # iterate over the bins\n",
        "    # select the items where the predicted max probability falls in the bin\n",
        "    # [tau_tab[i], tau_tab[i + 1)]\n",
        "    sec = (tau_tab[i + 1] > conf) & (conf >= tau_tab[i])\n",
        "    nb_items_bin[i] = np.sum(sec)  # Number of items in the bin\n",
        "    # select the predicted classes, and the true classes\n",
        "    class_pred_sec, y_sec = class_pred[sec], y[sec]\n",
        "    # average of the predicted max probabilities\n",
        "    mean_conf[i] = np.mean(conf[sec]) if nb_items_bin[i] > 0 else np.nan\n",
        "    # compute the empirical confidence\n",
        "    acc_tab[i] = np.mean(\n",
        "        class_pred_sec == y_sec) if nb_items_bin[i] > 0 else np.nan\n",
        "\n",
        "  # Cleaning\n",
        "  mean_conf = mean_conf[nb_items_bin > 0]\n",
        "  acc_tab = acc_tab[nb_items_bin > 0]\n",
        "  nb_items_bin = nb_items_bin[nb_items_bin > 0]\n",
        "\n",
        "  # Expected Calibration Error\n",
        "  ece = np.average(\n",
        "      np.absolute(mean_conf - acc_tab),\n",
        "      weights=nb_items_bin.astype(float) / np.sum(nb_items_bin))\n",
        "  # Maximum Calibration Error\n",
        "  mce = np.max(np.absolute(mean_conf - acc_tab))\n",
        "  return ece, mce\n",
        "\n",
        "\n",
        "def ensemble_metrics(x,\n",
        "                     y,\n",
        "                     model,\n",
        "                     log_likelihood_fn,\n",
        "                     n_samples=1,\n",
        "                     weight_files=None):\n",
        "  \"\"\"Evaluate metrics of an ensemble.\n",
        "\n",
        "  Args:\n",
        "    x: numpy array of inputs\n",
        "    y: numpy array of labels\n",
        "    model: tf.keras.Model.\n",
        "    log_likelihood_fn: keras function of log likelihood. For classification\n",
        "      tasks, log_likelihood_fn(...)[1] should return the logits\n",
        "    n_samples: number of Monte Carlo samples to draw per ensemble member (each\n",
        "      weight file).\n",
        "    weight_files: to draw samples from multiple weight sets, specify a list of\n",
        "      weight files to load. These files must have been generated through\n",
        "      keras's model.save_weights(...).\n",
        "\n",
        "  Returns:\n",
        "    metrics_dict: dictionary containing the metrics\n",
        "  \"\"\"\n",
        "  if weight_files is None:\n",
        "    ensemble_logprobs = [log_likelihood_fn([x, y])[0] for _ in range(n_samples)]\n",
        "    metric_values = [model.evaluate(x, y, verbose=0)\n",
        "                     for _ in range(n_samples)]\n",
        "    ensemble_logits = [log_likelihood_fn([x, y])[1] for _ in range(n_samples)]\n",
        "  else:\n",
        "    ensemble_logprobs = []\n",
        "    metric_values = []\n",
        "    ensemble_logits = []\n",
        "    for filename in weight_files:\n",
        "      model.load_weights(filename)\n",
        "      ensemble_logprobs.extend([log_likelihood_fn([x, y])[0]\n",
        "                                for _ in range(n_samples)])\n",
        "      ensemble_logits.extend([log_likelihood_fn([x, y])[1]\n",
        "                              for _ in range(n_samples)])\n",
        "      metric_values.extend([model.evaluate(x, y, verbose=0)\n",
        "                            for _ in range(n_samples)])\n",
        "\n",
        "  metric_values = np.mean(np.array(metric_values), axis=0)\n",
        "  results = {}\n",
        "  for m, name in zip(metric_values, model.metrics_names):\n",
        "    results[name] = m\n",
        "\n",
        "  ensemble_logprobs = np.array(ensemble_logprobs)\n",
        "  probabilistic_log_likelihood = np.mean(\n",
        "      scipy.special.logsumexp(\n",
        "          np.sum(ensemble_logprobs, axis=2)\n",
        "          if len(ensemble_logprobs.shape) > 2 else ensemble_logprobs,\n",
        "          b=1. / ensemble_logprobs.shape[0],\n",
        "          axis=0),\n",
        "      axis=0)\n",
        "  results['probabilistic_log_likelihood'] = probabilistic_log_likelihood\n",
        "\n",
        "  ensemble_logits = np.array(ensemble_logits)\n",
        "  probs = np.mean(scipy.special.softmax(ensemble_logits, axis=2), axis=0)\n",
        "  class_pred = np.argmax(probs, axis=1)\n",
        "  probabilistic_accuracy = np.mean(np.equal(y, class_pred))\n",
        "  results['probabilistic_accuracy'] = probabilistic_accuracy\n",
        "  results['ece'], results['mce'] = calibration(\n",
        "      one_hot(y, probs.shape[1]), probs)\n",
        "  results['brier_score'] = brier_score(one_hot(y, probs.shape[1]), probs)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt8RNXhGmQiz"
      },
      "source": [
        "# Adv Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP4rEtK-tP1D",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# epsilon = 0.3\n",
        "# adversary = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IKjNbpwnwg9h",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title AutoAttack Test the model (commented out)\n",
        "# # In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "# test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "#                                            batch_size = 1,\n",
        "#                                            shuffle = False)  \n",
        "# with torch.no_grad():\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for images, labels in tqdm(test_loader):\n",
        "#         images = images.to(device)\n",
        "#         labels = labels.to(device)\n",
        "#         x_adv = adversary.run_standard_evaluation(images, labels)\n",
        "#         outputs = model(x_adv)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_iKYSc8HO-Z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pip install cleverhans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgfFisa7HUn-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from cleverhans.torch.attacks.projected_gradient_descent import projected_gradient_descent as pgd\n",
        "from cleverhans.torch.attacks.carlini_wagner_l2 import carlini_wagner_l2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO0xPQlqXpqi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af9gk9p7z6Fu",
        "outputId": "04e8319e-4b1b-4c97-b96f-185d074fe4f4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title adv_acc 300\n",
        "# from re import X\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "set_seeds(42)\n",
        "pgd_params = {'eps': 0.1, 'eps_iter': 0.01, 'nb_iter': 20, 'norm': np.inf, 'targeted': False, 'rand_init': True} #, 'rand_minmax': 0.1}\n",
        "cw_params = {' n_classes':10, 'targeted': False}\n",
        "correct = 0\n",
        "xcorrect = 0\n",
        "total = 0\n",
        "distances = []\n",
        "xtest_dataset = torch.utils.data.Subset(test_dataset, np.arange(1000))\n",
        "test_loader = torch.utils.data.DataLoader(dataset = xtest_dataset,\n",
        "                                           batch_size = 1,\n",
        "                                           shuffle = False)  \n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    x_adv = pgd(model, images, **pgd_params)\n",
        "    outputs = model(images)\n",
        "    xoutputs = model(x_adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    _, xpredicted = torch.max(xoutputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    xcorrect += (xpredicted == labels).sum().item()\n",
        "    # print()\n",
        "    # show(images, predicted, labels, 'clean')\n",
        "    # show(x_adv, xpredicted, labels, 'adv')\n",
        "    # print('---')\n",
        "print('Accuracy of the network on {} test images: {} %'.format(total, 100 * correct / total))\n",
        "print('Accuracy of the network on {} test images: {} %'.format(total, 100 * xcorrect / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2wkzsrhaNKZ",
        "outputId": "62f0c7f9-2006-41ce-a7a6-c8bd0c528eb5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title adv_acc 300\n",
        "# from re import X\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "set_seeds(42)\n",
        "pgd_params = {'eps': 0.1, 'eps_iter': 0.01, 'nb_iter': 20, 'norm': np.inf, 'targeted': False, 'rand_init': True} #, 'rand_minmax': 0.1}\n",
        "cw_params = {'n_classes':10, 'targeted': False}\n",
        "correct = 0\n",
        "xcorrect = 0\n",
        "total = 0\n",
        "distances = []\n",
        "xtest_dataset = torch.utils.data.Subset(test_dataset, np.arange(1000))\n",
        "test_loader = torch.utils.data.DataLoader(dataset = xtest_dataset,\n",
        "                                           batch_size = 1,\n",
        "                                           shuffle = False)  \n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    x_adv = carlini_wagner_l2(model, images, **cw_params)\n",
        "    outputs = model(images)\n",
        "    xoutputs = model(x_adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    _, xpredicted = torch.max(xoutputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    xcorrect += (xpredicted == labels).sum().item()\n",
        "    # print()\n",
        "    # show(images, predicted, labels, 'clean')\n",
        "    # show(x_adv, xpredicted, labels, 'adv')\n",
        "    # print('---')\n",
        "print('Accuracy of the network on {} test images: {} %'.format(total, 100 * correct / total))\n",
        "print('Accuracy of the network on {} test images: {} %'.format(total, 100 * xcorrect / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i36LELQewY4X",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# #@title full 10k adv test\n",
        "# # Test the model\n",
        "# # In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "# set_seeds(42)\n",
        "# pgd_params = {'eps': 0.3, 'eps_iter': 0.1, 'nb_iter': 50, 'norm': np.inf, 'targeted': False, 'rand_init': True}\n",
        "\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# distances = []\n",
        "# test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "#                                            batch_size = 1,\n",
        "#                                            shuffle = False)  \n",
        "# for images, labels in tqdm(test_loader):\n",
        "#     images = images.to(device)\n",
        "#     labels = labels.to(device)\n",
        "#     x_adv = pgd(model, images,  **pgd_params)\n",
        "#     # assert(x_adv.shape==images.shape)\n",
        "#     outputs = model(x_adv)\n",
        "#     _, predicted = torch.max(outputs.data, 1)\n",
        "#     total += labels.size(0)\n",
        "#     correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pwdcZ9Dk0Ikd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title duplicate above with different args (eps_iter, rand_minmax)\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "set_seeds(42)\n",
        "pgd_params = {'eps': 0.3, 'eps_iter': 0.1, 'nb_iter': 50, 'norm': np.inf, 'targeted': False, 'rand_init': True, 'rand_minmax': 0.1}\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "distances = []\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = 1,\n",
        "                                           shuffle = False)  \n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    x_adv = pgd(model, images,  **pgd_params)\n",
        "    assert(x_adv.shape==images.shape)\n",
        "    outputs = model(x_adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MOJsX_--4LQ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# show(images, predicted, labels, 'clean')\n",
        "# show(x_adv, xpredicted, labels, 'clean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBssGxnRtTNw"
      },
      "source": [
        "# Adversarial Train Test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7c72eaMuWcm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "robust_model = LeNet5().to(device)\n",
        "\n",
        "#Setting the loss function\n",
        "cost = nn.CrossEntropyLoss()\n",
        "\n",
        "#Setting the optimizer with the model parameters and learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#this is defined to print how many steps are remaining when training\n",
        "total_step = len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUZH31-jHzJ-",
        "outputId": "672af048-2b59-4e9c-dffb-1736c15d8088",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title adversarial training\n",
        "pgd_params = {'eps': 0.1, 'eps_iter': 0.01, 'nb_iter': 20, 'norm': np.inf, 'targeted': False, 'rand_init': True} #, 'rand_minmax': 0.1}\n",
        "total_step = len(train_loader)\n",
        "robust_model.train()\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #attack\n",
        "        x_adv = pgd(robust_model, images,  **pgd_params)\n",
        "        delta = (images - x_adv).detach()\n",
        "        #Forward pass\n",
        "        outputs = model(images + delta)\n",
        "        loss = cost(outputs, labels)\n",
        "        \t\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \t\t\n",
        "        if (i+1) % 400 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "        \t\t           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8SfIjW2wsbU",
        "outputId": "c10b8f40-0418-4a42-dc7b-4f45a2095e75",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "robust_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-7LGsJQuUCx",
        "outputId": "03442b74-aad0-4fb2-d13a-eea6928e8842",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Clean Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = robust_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\t "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G5hPU6our4D",
        "outputId": "da833f41-1db4-45a4-d954-7aa390179e78",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title adv_acc 300\n",
        "# from re import X\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "set_seeds(42)\n",
        "pgd_params = {'eps': 0.1, 'eps_iter': 0.01, 'nb_iter': 20, 'norm': np.inf, 'targeted': False, 'rand_init': True} #, 'rand_minmax': 0.1}\n",
        "correct = 0\n",
        "xcorrect = 0\n",
        "total = 0\n",
        "distances = []\n",
        "xtest_dataset = torch.utils.data.Subset(test_dataset, np.arange(1000))\n",
        "test_loader = torch.utils.data.DataLoader(dataset = xtest_dataset,\n",
        "                                           batch_size = 1,\n",
        "                                          shuffle = False)  \n",
        "\n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    x_adv = pgd(model, images, **pgd_params)\n",
        "    xoutputs = robust_model(x_adv.detach())\n",
        "    _, xpredicted = torch.max(xoutputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    xcorrect += (xpredicted == labels).sum().item()\n",
        "    # print()\n",
        "    # show(images, predicted, labels, 'clean')\n",
        "    # show(x_adv, xpredicted, labels, 'adv')\n",
        "    # print('---')\n",
        "print('AAccuracy of the network on {} test images: {} %'.format(total, 100 * xcorrect / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBW-n4oQtcJv"
      },
      "source": [
        "# Others attacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4au7JwtVymxK",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title deefool\n",
        "def compute_norm(x, norm):\n",
        "    with torch.no_grad():\n",
        "        if norm == np.inf:\n",
        "            return torch.linalg.norm(torch.ravel(x.cpu()), ord=np.inf).numpy()\n",
        "        elif norm == 2:\n",
        "            return torch.linalg.norm(x.cpu()).numpy()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "def deepfool_attack(model, x, max_iter, **args):\n",
        "    \"\"\"DeepFool attack\"\"\"\n",
        "    nx = x.clone()\n",
        "    nx.requires_grad_()\n",
        "    eta = torch.zeros(nx.shape).cuda() if torch.cuda.is_available() else torch.zeros(nx.shape)\n",
        "\n",
        "    out = model(nx+eta)\n",
        "    n_class = out.shape[1]\n",
        "    initial_label = out.max(1)[1]\n",
        "    pred_nx = out.max(1)[1]\n",
        "\n",
        "    i_iter = 0\n",
        "    cumul_dis_2 = 0.\n",
        "    cumul_dis_inf = 0.\n",
        "    while pred_nx == initial_label and i_iter < max_iter:\n",
        "        out[0, pred_nx].backward(retain_graph=True)\n",
        "        grad_np = nx.grad.data.clone()\n",
        "        value_l = np.inf\n",
        "        w_l = None\n",
        "        for i in range(n_class):\n",
        "            if i == initial_label:\n",
        "                continue\n",
        "\n",
        "            nx.grad.data.zero_()\n",
        "            out[0, i].backward(retain_graph=True)\n",
        "            grad_i = nx.grad.data.clone()\n",
        "\n",
        "            wi = grad_i - grad_np\n",
        "            fi = out[0, i] - out[0, initial_label]\n",
        "            value_i = np.abs(fi.item()) / torch.norm(wi.flatten())\n",
        "            # breakpoint()\n",
        "            if value_i < value_l:\n",
        "                value_l = value_i\n",
        "                w_l = wi\n",
        "\n",
        "        ri = value_l/torch.norm(w_l.flatten()) * w_l\n",
        "        #\n",
        "        cumul_dis_inf += compute_norm(ri, norm=np.inf)\n",
        "        cumul_dis_2 += compute_norm(ri, norm=2)\n",
        "        #\n",
        "        eta += ri.clone()\n",
        "        nx.grad.data.zero_()\n",
        "        out = model(nx+eta)\n",
        "        pred_nx = out.max(1)[1]\n",
        "        i_iter += 1\n",
        "\n",
        "    cumul_dis = {'2': cumul_dis_2, 'inf': cumul_dis_inf}\n",
        "    return x+ri, i_iter, cumul_dis\n",
        "\n",
        "\n",
        "def test_deepfool_attack(model, x, y=None, **args):\n",
        "    \"\"\"DeepFool attack\"\"\"\n",
        "    return deepfool_attack(model, x, max_iter=args['nb_iter'])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPGRTHpvythu",
        "outputId": "277b1ee0-0a3f-4745-fda5-5a15eafcd788",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title test deepfool\n",
        "#@title adv_acc 300\n",
        "# from re import X\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "set_seeds(42)\n",
        "\n",
        "correct = 0\n",
        "xcorrect = 0\n",
        "total = 0\n",
        "distances = []\n",
        "xtest_dataset = torch.utils.data.Subset(test_dataset, np.arange(1000))\n",
        "test_loader = torch.utils.data.DataLoader(dataset = xtest_dataset,\n",
        "                                           batch_size = 1,\n",
        "                                           shuffle = False)\n",
        "args = {'nb_iter': 100}  \n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    x_adv = test_deepfool_attack(model, images, **args)\n",
        "    outputs = model(images)\n",
        "    xoutputs = model(x_adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    _, xpredicted = torch.max(xoutputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    xcorrect += (xpredicted == labels).sum().item()\n",
        "    # print()\n",
        "    # show(images, predicted, labels, 'clean')\n",
        "    # show(x_adv, xpredicted, labels, 'adv')\n",
        "    # print('---')\n",
        "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * xcorrect / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm3BjYHqFckA",
        "outputId": "39e79eaf-8958-4664-8b40-8e4b73081df1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title test autoattack\n",
        "#@title adv_acc 300\n",
        "# from re import X\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "set_seeds(42)\n",
        "\n",
        "correct = 0\n",
        "xcorrect = 0\n",
        "total = 0\n",
        "distances = []\n",
        "xtest_dataset = torch.utils.data.Subset(test_dataset, np.arange(1000))\n",
        "bs = 100\n",
        "test_loader = torch.utils.data.DataLoader(dataset = xtest_dataset,\n",
        "                                           batch_size = bs,\n",
        "                                           shuffle = False)\n",
        "epsilon = 0.3\n",
        "adversary = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)\n",
        "\n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    x_adv = adversary.run_standard_evaluation(images, labels, bs=bs)\n",
        "    outputs = model(images)\n",
        "    xoutputs = model(x_adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    _, xpredicted = torch.max(xoutputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    xcorrect += (xpredicted == labels).sum().item()\n",
        "    # print()\n",
        "    # show(images, predicted, labels, 'clean')\n",
        "    # show(x_adv, xpredicted, labels, 'adv')\n",
        "    # print('---')\n",
        "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "print('AAccuracy of the network on the 10000 test images: {} %'.format(100 * xcorrect / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "foJPEEIQ0yWu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title pgd_adaptive\n",
        "\"\"\"The Projected Gradient Descent attack.\n",
        "Original from cleverhans: https://github.com/cleverhans-lab/cleverhans/blob/master/cleverhans/torch/attacks/projected_gradient_descent.py\n",
        "Edited by: Jonas Ngnawe\n",
        "\"\"\"\n",
        "\n",
        "from cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method\n",
        "from cleverhans.torch.utils import clip_eta\n",
        "\n",
        "\n",
        "def compute_norm(x, norm):\n",
        "    if norm == np.inf:\n",
        "        return torch.linalg.norm(torch.ravel(x.cpu()), ord=np.inf)\n",
        "    elif norm == 2:\n",
        "        return torch.linalg.norm(x.cpu())\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "def xpgd(\n",
        "    model_fn,\n",
        "    x,\n",
        "    eps,\n",
        "    eps_iter,\n",
        "    nb_iter,\n",
        "    norm,\n",
        "    clip_min=None,\n",
        "    clip_max=None,\n",
        "    y=None,\n",
        "    targeted=False,\n",
        "    rand_init=True,\n",
        "    rand_minmax=None,\n",
        "    sanity_checks=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    This class implements either the Basic Iterative Method\n",
        "    (Kurakin et al. 2016) when rand_init is set to False. or the\n",
        "    Madry et al. (2017) method if rand_init is set to True.\n",
        "    Paper link (Kurakin et al. 2016): https://arxiv.org/pdf/1607.02533.pdf\n",
        "    Paper link (Madry et al. 2017): https://arxiv.org/pdf/1706.06083.pdf\n",
        "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
        "    :param x: input tensor.\n",
        "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
        "    :param eps_iter: step size for each attack iteration\n",
        "    :param nb_iter: Number of attack iterations.\n",
        "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf, 1 or 2.\n",
        "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
        "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
        "    :param y: (optional) Tensor with true labels. If targeted is true, then provide the\n",
        "              target label. Otherwise, only provide this parameter if you'd like to use true\n",
        "              labels when crafting adversarial samples. Otherwise, model predictions are used\n",
        "              as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
        "              https://arxiv.org/abs/1611.01236). Default is None.\n",
        "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
        "              Untargeted, the default, will try to make the label incorrect.\n",
        "              Targeted will instead try to move in the direction of being more like y.\n",
        "    :param rand_init: (optional) bool. Whether to start the attack from a randomly perturbed x.\n",
        "    :param rand_minmax: (optional) bool. Support of the continuous uniform distribution from\n",
        "              which the random perturbation on x was drawn. Effective only when rand_init is\n",
        "              True. Default equals to eps.\n",
        "    :param sanity_checks: bool, if True, include asserts (Turn them off to use less runtime /\n",
        "              memory or for unit tests that intentionally pass strange input)\n",
        "    :return: a tensor for the adversarial example\n",
        "    \"\"\"\n",
        "    if norm == 1:\n",
        "        raise NotImplementedError(\n",
        "            \"It's not clear that FGM is a good inner loop\"\n",
        "            \" step for PGD when norm=1, because norm=1 FGM \"\n",
        "            \" changes only one pixel at a time. We need \"\n",
        "            \" to rigorously test a strong norm=1 PGD \"\n",
        "            \"before enabling this feature.\"\n",
        "        )\n",
        "    if norm not in [np.inf, 2]:\n",
        "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
        "    if eps < 0:\n",
        "        raise ValueError(\n",
        "            \"eps must be greater than or equal to 0, got {} instead\".format(\n",
        "                eps)\n",
        "        )\n",
        "    if eps == 0:\n",
        "        return x\n",
        "    if eps_iter < 0:\n",
        "        raise ValueError(\n",
        "            \"eps_iter must be greater than or equal to 0, got {} instead\".format(\n",
        "                eps_iter\n",
        "            )\n",
        "        )\n",
        "    if eps_iter == 0:\n",
        "        return x\n",
        "\n",
        "    assert eps_iter <= eps, (eps_iter, eps)\n",
        "    if clip_min is not None and clip_max is not None:\n",
        "        if clip_min > clip_max:\n",
        "            raise ValueError(\n",
        "                \"clip_min must be less than or equal to clip_max, got clip_min={} and clip_max={}\".format(\n",
        "                    clip_min, clip_max\n",
        "                )\n",
        "            )\n",
        "\n",
        "    asserts = []\n",
        "\n",
        "    # If a data range was specified, check that the input was in that range\n",
        "    if clip_min is not None:\n",
        "        assert_ge = torch.all(\n",
        "            torch.ge(x, torch.tensor(clip_min, device=x.device, dtype=x.dtype))\n",
        "        )\n",
        "        asserts.append(assert_ge)\n",
        "\n",
        "    if clip_max is not None:\n",
        "        assert_le = torch.all(\n",
        "            torch.le(x, torch.tensor(clip_max, device=x.device, dtype=x.dtype))\n",
        "        )\n",
        "        asserts.append(assert_le)\n",
        "\n",
        "    # Initialize loop variables\n",
        "    if rand_init:\n",
        "        if rand_minmax is None:\n",
        "            rand_minmax = eps\n",
        "        eta = torch.zeros_like(x).uniform_(-rand_minmax, rand_minmax)\n",
        "    else:\n",
        "        eta = torch.zeros_like(x)\n",
        "\n",
        "    # Clip eta\n",
        "    eta = clip_eta(eta, norm, eps)\n",
        "    adv_x = x + eta\n",
        "    if clip_min is not None or clip_max is not None:\n",
        "        adv_x = torch.clamp(adv_x, clip_min, clip_max)\n",
        "\n",
        "    if y is None:\n",
        "        # Using model predictions as ground truth to avoid label leaking\n",
        "        _, y = torch.max(model_fn(x), 1)\n",
        "\n",
        "    i = 0\n",
        "    cumul_dis = 0.\n",
        "    # Jonas: nb_iter will now act as max number of iterations (supposed very high)\n",
        "    while i < nb_iter and torch.max(model_fn(adv_x), 1)[1] == y:\n",
        "        tmp_x = adv_x.clone()\n",
        "        adv_x = fast_gradient_method(\n",
        "            model_fn,\n",
        "            adv_x,\n",
        "            eps_iter,\n",
        "            norm,\n",
        "            clip_min=clip_min,\n",
        "            clip_max=clip_max,\n",
        "            y=y,\n",
        "            targeted=targeted,\n",
        "        )\n",
        "        #\n",
        "        delta = adv_x - tmp_x\n",
        "        cumul_dis += compute_norm(delta, norm)\n",
        "        # Clipping perturbation eta to norm norm ball\n",
        "        eta = adv_x - x\n",
        "        eta = clip_eta(eta, norm, eps)\n",
        "        adv_x = x + eta\n",
        "\n",
        "        # Redo the clipping.\n",
        "        # FGM already did it, but subtracting and re-adding eta can add some\n",
        "        # small numerical error.\n",
        "        if clip_min is not None or clip_max is not None:\n",
        "            adv_x = torch.clamp(adv_x, clip_min, clip_max)\n",
        "        i += 1\n",
        "\n",
        "    asserts.append(eps_iter <= eps)\n",
        "    if norm == np.inf and clip_min is not None:\n",
        "        # TODO necessary to cast clip_min and clip_max to x.dtype?\n",
        "        asserts.append(eps + clip_min <= clip_max)\n",
        "\n",
        "    if sanity_checks:\n",
        "        assert np.all(asserts)\n",
        "\n",
        "    return adv_x, i, cumul_dis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YY_HOXGL0oc0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title eval dis\n",
        "# from re import X\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "set_seeds(42)\n",
        "pgd_params = {'eps': 0.3, 'eps_iter': 0.1, 'nb_iter': 20, 'norm': np.inf, 'targeted': False, 'rand_init': True}\n",
        "\n",
        "correct = 0\n",
        "xcorrect = 0\n",
        "total = 0\n",
        "distances = []\n",
        "xtest_dataset = torch.utils.data.Subset(test_dataset, np.arange(300))\n",
        "test_loader = torch.utils.data.DataLoader(dataset = xtest_dataset,\n",
        "                                           batch_size = 1,\n",
        "                                           shuffle = False)  \n",
        "dis_list = []\n",
        "iter_list = []\n",
        "k=0\n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    x_adv, i, cumul_dis= xpgd(model, images, **pgd_params)\n",
        "    if torch.is_tensor(cumul_dis):\n",
        "          cumul_dis = cumul_dis.detach().numpy()\n",
        "    dis_list.append(cumul_dis)\n",
        "    iter_list.append(i)\n",
        "    \n",
        "    # if i == 0:\n",
        "    #   k = k+1\n",
        "    # if k == 5:\n",
        "    #   break\n",
        "    outputs = model(images)\n",
        "    xoutputs = model(x_adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    _, xpredicted = torch.max(xoutputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    xcorrect += (xpredicted == labels).sum().item()\n",
        "    # print()\n",
        "    # show(images, predicted, labels, 'clean')\n",
        "    # show(x_adv, xpredicted, labels, 'adv')\n",
        "    # print('---')\n",
        "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * xcorrect / total))\n",
        "print(np.mean(dis_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-bEflPE26A7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# show(images, predicted, labels, 'clean')\n",
        "# show(x_adv, xpredicted, labels, 'clean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9U8TS_y3q6S",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# eps=0.3\n",
        "# rand_minmax = eps\n",
        "# eta = torch.zeros_like(images).uniform_(-rand_minmax, rand_minmax)\n",
        "# eta = clip_eta(eta, np.inf, eps)\n",
        "# adv_x = images + eta\n",
        "# _, xxpredicted = torch.max(xoutputs.data, 1)\n",
        "# show(adv_x, xxpredicted, predicted, 'adv_x')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5Z0OFtZ2Uk9",
        "outputId": "39115f72-e70e-4519-8e50-ac6e349756b6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "iter_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5f7H1zGIrWr1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title study function\n",
        "def adv_acc_var(eps_iter, nb_iter=20):\n",
        "  set_seeds(42)\n",
        "  pgd_params = {'eps': 0.3, 'eps_iter': eps_iter, 'nb_iter': nb_iter, 'norm': np.inf, 'targeted': False, 'rand_init': True}\n",
        "\n",
        "  correct = 0\n",
        "  xcorrect = 0\n",
        "  total = 0\n",
        "  distances = []\n",
        "  xtest_dataset = torch.utils.data.Subset(test_dataset, np.arange(100))\n",
        "  test_loader = torch.utils.data.DataLoader(dataset = xtest_dataset,\n",
        "                                            batch_size = 1,\n",
        "                                            shuffle = False)  \n",
        "  for images, labels in tqdm(test_loader):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      x_adv = pgd(model, images, y = labels, **pgd_params)\n",
        "      outputs = model(images)\n",
        "      xoutputs = model(x_adv)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      _, xpredicted = torch.max(xoutputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      xcorrect += (xpredicted == labels).sum().item()\n",
        "      # print()\n",
        "      # show(images, predicted, labels, 'clean')\n",
        "      # show(x_adv, xpredicted, labels, 'adv')\n",
        "      # print('---')\n",
        "  acc = 100 * correct / total\n",
        "  adv_acc = 100 * xcorrect / total \n",
        "  # print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "  # print('Accuracy of the network on the 10000 test images: {} %'.format(100 * xcorrect / total))\n",
        "  return acc, adv_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "IoMn6j7BsAwx",
        "outputId": "facd1f2b-1baf-4f8a-a57c-e28e0cfc77ff",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "eps_iter_list = [0.001, 0.01, 0.05, 0.1]\n",
        "nb_iter_list = [20]\n",
        "accL=[]\n",
        "adv_accL = []\n",
        "# k = 0\n",
        "for eps_iter in eps_iter_list:\n",
        "  for nb_iter in nb_iter_list:\n",
        "    acc, adv_acc = adv_acc_var(eps_iter, nb_iter)\n",
        "    accL.append(acc)\n",
        "    adv_accL.append(adv_acc)\n",
        "\n",
        "plt.plot(accL)\n",
        "plt.figure()\n",
        "plt.plot(adv_accL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkDWmNBlw_gr",
        "outputId": "c4ba96ae-7872-4c07-b123-1ffc406b6ebb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "adv_accL, accL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOnDuw8wuuz6",
        "outputId": "237ba381-c9fe-4d9d-f366-6ed0f4650233",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "list(zip(eps_iter_list, nb_iter_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "xE01LP9Pt7l0",
        "outputId": "e7eb3515-f2cb-4c08-92e0-38e79841a68f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "eps_iter_list = [0.001, 0.01, 0.05, 0.1]\n",
        "nb_iter_list = [500, 200, 50, 20]\n",
        "accL=[]\n",
        "adv_accL = []\n",
        "for eps_iter, nb_iter in list(zip(eps_iter_list, nb_iter_list)):\n",
        "    acc, adv_acc = adv_acc_var(eps_iter, nb_iter)\n",
        "    accL.append(acc)\n",
        "    adv_accL.append(adv_acc)\n",
        "\n",
        "plt.plot(eps_iter_list, accL)\n",
        "plt.figure()\n",
        "plt.plot(eps_iter_list, adv_accL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G00WQZS0x1oP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def show(im, predicted, label, txt):\n",
        "# im1 = images[0][0].cpu().numpy()\n",
        "  plt.figure()\n",
        "  im = im[0][0].detach().cpu().numpy()\n",
        "  plt.imshow(im)\n",
        "  # print(predicted.cpu().numpy(), label.cpu().numpy(), (label==predicted).cpu().numpy())\n",
        "  title = txt+' {} {} {}'.format(predicted.cpu().numpy(), label.cpu().numpy(), (label==predicted).cpu().numpy())\n",
        "  plt.title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM_ultE1sbya"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA4ZvuJucvDs",
        "outputId": "c2d4310f-611c-4aa5-c232-a2e9a502f860",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pip install foolbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHdJoZE-dNtg",
        "outputId": "91b93907-326b-464e-e379-74f5a462b9b8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import foolbox as fb\n",
        "\n",
        "fmodel = fb.PyTorchModel(model, bounds=(-5, 5))\n",
        "attack = fb.attacks.LinfPGD()\n",
        "epsilons = [0.3]\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "pgd_params = {'eps': 0.3, 'eps_iter': 0.05, 'nb_iter': 50, 'norm': np.inf, 'targeted': False, 'rand_init': True}\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    _, advs, success = attack(fmodel, images, labels, epsilons=epsilons)\n",
        "    total += labels.size(0)\n",
        "    correct += success.sum().item()\n",
        "\n",
        "print('FAccuracy of the network on the 10000 test images: {} %'.format(100 * (1 - correct) / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-owEylAkhVS",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# show(advs[0], predicted, labels, 'x')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB-_RI9bQ6EF",
        "outputId": "20d60216-fd21-49dc-fc74-f226f1023219",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "pgd_params = {'eps': 0.3, 'eps_iter': 0.05, 'nb_iter': 50, 'norm': np.inf, 'targeted': False, 'rand_init': True}\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    x_adv = pgd(model, images, y=labels, **pgd_params)\n",
        "    outputs = model(x_adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Anh0I42jNIu",
        "outputId": "aa8fecbe-c6d8-4d49-f4b8-e3e3ca141d4a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "pgd_params = {'eps': 0.3, 'eps_iter': 0.05, 'nb_iter': 100, 'norm': np.inf, 'targeted': False, 'rand_init': True}\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    x_adv = pgd(model, images, y=labels, **pgd_params)\n",
        "    outputs = model(x_adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBJSpCmYz0aE",
        "outputId": "8799a0d8-6201-4d3c-f0e8-f8f2e69ec120",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "pgd_params = {'eps': 0.3, 'eps_iter': 0.1, 'nb_iter': 20, 'norm': np.inf, 'targeted': False, 'rand_init': True}\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = 1,\n",
        "                                           shuffle = False)\n",
        "for images, labels in tqdm(test_loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    x_adv = pgd(model, images, y=labels, **pgd_params)\n",
        "    outputs = model(x_adv)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    \n",
        "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx1qIyRktwxc"
      },
      "source": [
        "# Distances eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9X-TA523A9x"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "G5LfyvOp8C7g",
        "outputId": "1e6cfd08-82f6-4639-f540-26e29733219b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "test_dataset.data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "eAxdkuRj60y3",
        "outputId": "59d8b54e-7afa-4771-a846-25c4a6002bbe",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# We want to get TSNE embedding with 2 dimensions\n",
        "N=100\n",
        "idxs = np.arange(len(test_dataset))\n",
        "random.shuffle(idxs)\n",
        "idxs = idxs[:N]\n",
        "n_components = 2\n",
        "tsne = TSNE(n_components,  learning_rate='auto', init='random', perplexity=30)\n",
        "tsne_result = tsne.fit_transform(test_dataset.data[idxs].reshape(N,-1)/255.)\n",
        "tsne_result.shape\n",
        "# (1000, 2)\n",
        "# Two dimensions for each of our images\n",
        " \n",
        "# Plot the result of our TSNE with the label color coded\n",
        "# A lot of the stuff here is about making the plot look pretty and not TSNE\n",
        "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': y})\n",
        "fig, ax = plt.subplots(1)\n",
        "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_result_df, ax=ax,s=120)\n",
        "lim = (tsne_result.min()-5, tsne_result.max()+5)\n",
        "ax.set_xlim(lim)\n",
        "ax.set_ylim(lim)\n",
        "ax.set_aspect('equal')\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiVbFLu03Erd",
        "outputId": "c4eff72a-de4e-44b5-d63d-4e2592cca99f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.manifold import TSNE\n",
        "X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(X)\n",
        "X_embedded.shape, X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmtHD0am39QU"
      },
      "source": [
        "# Metrics/ART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-U9Qj9o4AE8",
        "outputId": "3799e57a-f130-4087-ebf8-586d804f16ee",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pip install adversarial-robustness-toolbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiWzrm7X4DcO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import art\n",
        "from art.estimators.classification.pytorch import PyTorchClassifier\n",
        "from art.metrics.metrics import empirical_robustness, clever_t, clever_u, clever, loss_sensitivity, wasserstein_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huqF2wQKmo49",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from art.metrics.metrics import empirical_robustness, clever_t, clever_u, clever, loss_sensitivity, wasserstein_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI-rk7bL_sf1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "89762698f01e41c2968634676440fb15",
            "e4ef0a26f1324bd9a86b92d8346f4dd4",
            "f816a3afc67c4db09024f7bdb7ff59dc",
            "2738125907144cff9e228ccd6f6e4138",
            "0504a26627304d1b929dd3cba0f46384",
            "5124a2c9c46948f487c4b9d0ba6b6ac7",
            "92b6cd50272b4f11a4197e1a3381b6b4",
            "9b693b0b17214bb087e78d6a0fdc9a21",
            "86095b75c15a4e0cbc033194d1e8539e",
            "077ea55876d14242bdc7ae5906c5f621",
            "5e7ab31da228492d8950042fa7f6c3cd"
          ]
        },
        "id": "qaNKNuky5fxA",
        "outputId": "5e106052-cf8b-4ed6-acaf-18abaa99dac9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = 1,\n",
        "                                           shuffle = False)  \n",
        "\n",
        "for images, labels in test_loader:\n",
        "  min_pixel_value, max_pixel_value = images.min(), images.max()\n",
        "  break\n",
        "# Get the classifier\n",
        "classifier = PyTorchClassifier(\n",
        "    model=model,\n",
        "    clip_values=(min_pixel_value, max_pixel_value),\n",
        "    loss=None,\n",
        "    optimizer=None,\n",
        "    input_shape=(1, 32, 32),\n",
        "    nb_classes=10,\n",
        ")\n",
        "# classifier = copy.deepcopy(model)\n",
        "# Get the classifier\n",
        "# krc.fit(x_train, y_train, batch_size=batch_size, nb_epochs=1, verbose=0)\n",
        "R_Linf = 0.3\n",
        "R_L2 = 2\n",
        "\n",
        "# Test targeted clever\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = 1,\n",
        "                                           shuffle = False)\n",
        "for images, labels in test_loader:\n",
        "  # images, labels = images.to(device), labels.to(device)\n",
        "  predictions = classifier.predict(images.cpu())\n",
        "  print((predictions.shape, np.argmax(predictions, axis=1) == labels.numpy()))\n",
        "  accuracy = np.sum(np.argmax(predictions, axis=1) == labels.numpy()) / len(labels)\n",
        "  # accuracy = (predictions.max(1)[1] == labels).sum() / len(labels)\n",
        "\n",
        "  print(\"Accuracy: {}%\".format(accuracy * 100))\n",
        "\n",
        "  # x_test = images.clone()\n",
        "  # Step 6: Generate adversarial test examples\n",
        "  # attack = FastGradientMethod(estimator=model, eps=0.2)\n",
        "  # x_test_adv = attack.generate(x=x_test)\n",
        "\n",
        "  # Step 7: Evaluate the ART classifier on adversarial test examples\n",
        "\n",
        "  # predictions = model.predict(x_test_adv)\n",
        "  # accuracy = np.sum(np.argmax(predictions, axis=1) == labels.numpy()) / len(labels)\n",
        "  # print(\"Accuracy on adversarial test examples: {}%\".format(accuracy * 100))\n",
        "  # break\n",
        "\n",
        "  res0 = clever_t(classifier, images[0].numpy(), target_class=2, nb_batches=10, batch_size=5, radius=R_Linf, norm=np.inf, pool_factor=3)\n",
        "  print(\"Targeted CLEVER score:\",res0)\n",
        "  res1 = clever_u(classifier, images[0].numpy(), nb_batches=10, batch_size=5, radius=R_Linf, norm=np.inf, pool_factor=3)\n",
        "  print(\"Untargeted CLEVER score:\",res1)\n",
        "  break\n",
        "\n",
        "\n",
        "# # classifier = model\n",
        "# for images, labels in test_loader:\n",
        "#     # images = images.to(device)\n",
        "#     # labels = labels.to(device)\n",
        "#     print(images.shape)\n",
        "#     # y = np.array(images)\n",
        "#     # print(y.shape)\n",
        "#     a = art.metrics.clever_u(model, images, nb_batches=2, batch_size=1, radius=0.3, norm=np.inf, verbose = True)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0Q2GFDjGJzb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from art.estimators.classification.pytorch import PyTorchClassifier\n",
        "from art.metrics.metrics import empirical_robustness, clever_t, clever_u, clever, loss_sensitivity, wasserstein_distance\n",
        "classifier = PyTorchClassifier(\n",
        "  model=model,\n",
        "  clip_values=(min_pixel_value, max_pixel_value),\n",
        "  loss=None,\n",
        "  optimizer=None,\n",
        "  input_shape=(1, 32, 32),\n",
        "  nb_classes=10,\n",
        ")\n",
        "res1 = clever_u(classifier, images[0].numpy(), nb_batches=10, batch_size=5, radius=R_Linf, norm=np.inf, pool_factor=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzmQ8PrMfjia",
        "outputId": "f2c89a5a-5d51-47c7-bef6-f34a5d6f151f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The script demonstrates a simple example of using ART with PyTorch. The example train a small model on the MNIST dataset\n",
        "and creates adversarial examples using the Fast Gradient Sign Method. Here we use the ART classifier to train the model,\n",
        "it would also be possible to provide a pretrained model to the ART classifier.\n",
        "The parameters are chosen for reduced computational requirements of the script and not optimised for accuracy.\n",
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from art.utils import load_mnist\n",
        "\n",
        "\n",
        "# Step 0: Define the neural network model, return logits instead of activation in forward method\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=1)\n",
        "        self.conv_2 = nn.Conv2d(in_channels=4, out_channels=10, kernel_size=5, stride=1)\n",
        "        self.fc_1 = nn.Linear(in_features=4 * 4 * 10, out_features=100)\n",
        "        self.fc_2 = nn.Linear(in_features=100, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv_1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv_2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4 * 4 * 10)\n",
        "        x = F.relu(self.fc_1(x))\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Step 1: Load the MNIST dataset\n",
        "\n",
        "(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = load_mnist()\n",
        "\n",
        "# Step 1a: Swap axes to PyTorch's NCHW format\n",
        "\n",
        "x_train = np.transpose(x_train, (0, 3, 1, 2)).astype(np.float32)\n",
        "x_test = np.transpose(x_test, (0, 3, 1, 2)).astype(np.float32)\n",
        "\n",
        "# Step 2: Create the model\n",
        "\n",
        "xmodel = Net()\n",
        "\n",
        "# Step 2a: Define the loss function and the optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Step 3: Create the ART classifier\n",
        "\n",
        "classifier = PyTorchClassifier(\n",
        "    model=xmodel,\n",
        "    clip_values=(min_pixel_value, max_pixel_value),\n",
        "    loss=criterion,\n",
        "    optimizer=optimizer,\n",
        "    input_shape=(1, 28, 28),\n",
        "    nb_classes=10,\n",
        ")\n",
        "\n",
        "# Step 4: Train the ART classifier\n",
        "\n",
        "classifier.fit(x_train, y_train, batch_size=64, nb_epochs=3)\n",
        "\n",
        "# Step 5: Evaluate the ART classifier on benign test examples\n",
        "\n",
        "predictions = classifier.predict(x_test)\n",
        "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
        "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
        "\n",
        "# Step 6: Generate adversarial test examples\n",
        "attack = FastGradientMethod(estimator=classifier, eps=0.2)\n",
        "x_test_adv = attack.generate(x=x_test)\n",
        "\n",
        "# Step 7: Evaluate the ART classifier on adversarial test examples\n",
        "\n",
        "predictions = classifier.predict(x_test_adv)\n",
        "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
        "print(\"Accuracy on adversarial test examples: {}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142,
          "referenced_widgets": [
            "d4c45a6eaa644f6a9163e27940ad84eb",
            "db83947234ab43f98913f3bd77cf65a8",
            "92e9f350977049698d4418d86e467522",
            "8f154b8c1fc74bcfa75b7652537257ca",
            "59705729645744129ea854f5c529f747",
            "515ed075a5b24d40a6892fb7d2d278d9",
            "8ae8291bff194e57aafa8120bd2bdb4c",
            "5ef98528ae94479d8ac737ddb88bb988",
            "7aa88d56705a4c83a61cd5f5056e2ab0",
            "73213189a25c4db4943b95f5e8ceb7d6",
            "29e8cd966e874a6bb2f5200d91fdb923"
          ]
        },
        "id": "cJpiYvXa2bAf",
        "outputId": "1e7a1d96-5211-4f9a-f048-fd2c923c42a5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "import torch.optim as optim\n",
        "\n",
        "from art.estimators.classification.keras import KerasClassifier\n",
        "from art.estimators.classification.pytorch import PyTorchClassifier\n",
        "from art.estimators.classification.tensorflow import TensorFlowClassifier\n",
        "from art.metrics.metrics import empirical_robustness, clever_t, clever_u, clever, loss_sensitivity, wasserstein_distance\n",
        "from art.utils import load_mnist\n",
        "\n",
        "from tests.utils import master_seed\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "batch_size, nb_train, nb_test = 100, 1000, 10\n",
        "(x_train, y_train), (x_test, y_test), _, _ = load_mnist()\n",
        "x_train, y_train = x_train[:nb_train], y_train[:nb_train]\n",
        "x_test, y_test = x_test[:nb_test], y_test[:nb_test]\n",
        "tfmodel = Sequential()\n",
        "tfmodel.add(Conv2D(4, kernel_size=(5, 5), activation=\"relu\", input_shape=(28, 28, 1)))\n",
        "tfmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "tfmodel.add(Flatten())\n",
        "tfmodel.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "tfmodel.compile(\n",
        "loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Get the classifier\n",
        "krc = KerasClassifier(model=tfmodel, clip_values=(0, 1), use_logits=False)\n",
        "\n",
        "# Get the classifier\n",
        "krc.fit(x_train, y_train, batch_size=batch_size, nb_epochs=1, verbose=0)\n",
        "R_L1 = 40\n",
        "R_L2 = 2\n",
        "\n",
        "# Test targeted clever\n",
        "res0 = clever_t(krc, x_test[-1], 2, 10, 5, R_L1, norm=1, pool_factor=3)\n",
        "print(\"Targeted CLEVER score:\",res0)\n",
        "res1 = clever_u(krc, x_test[-1], 10, 5, R_L1, norm=1, pool_factor=3)\n",
        "print(\"Untargeted CLEVER score:\",res1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlV6ebARppBL",
        "outputId": "f2b3fe35-d3ad-4ea5-c068-6a7388bf4d8c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "x_test[-1].dtype, type(x_test[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_k9rbyQp_Zr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zBssGxnRtTNw",
        "jBW-n4oQtcJv"
      ],
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "vscode": {
      "interpreter": {
        "hash": "62fd54318f6a2fa08be4eeaf7e2f88489af61833aae47133177ad67efbdf279b"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0504a26627304d1b929dd3cba0f46384": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0656fda51c7345ea9567e55ccd287894": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d9be0b924294f2e97b72d551d3ee698",
            "placeholder": "",
            "style": "IPY_MODEL_58f32065966847688ee268ee804212f1",
            "value": "100%"
          }
        },
        "077ea55876d14242bdc7ae5906c5f621": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09a2297ed05e4418a70571ce467a06d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0656fda51c7345ea9567e55ccd287894",
              "IPY_MODEL_d7fd83c4f42f41be9636356b6e57403e",
              "IPY_MODEL_ee4e4045a51649baae806d2dcc3f88a1"
            ],
            "layout": "IPY_MODEL_a4a1f5cf2b004536bfcfac2c044b66cc"
          }
        },
        "0d9be0b924294f2e97b72d551d3ee698": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f8ba678bd94de790854d597858fa89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2738125907144cff9e228ccd6f6e4138": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_077ea55876d14242bdc7ae5906c5f621",
            "placeholder": "",
            "style": "IPY_MODEL_5e7ab31da228492d8950042fa7f6c3cd",
            "value": " 9/9 [00:00&lt;00:00, 18.14it/s]"
          }
        },
        "29e8cd966e874a6bb2f5200d91fdb923": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44c2240be0644591aaae680e02388df0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5124a2c9c46948f487c4b9d0ba6b6ac7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "515ed075a5b24d40a6892fb7d2d278d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58f32065966847688ee268ee804212f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59705729645744129ea854f5c529f747": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e7ab31da228492d8950042fa7f6c3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ef98528ae94479d8ac737ddb88bb988": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73213189a25c4db4943b95f5e8ceb7d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa88d56705a4c83a61cd5f5056e2ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86095b75c15a4e0cbc033194d1e8539e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89762698f01e41c2968634676440fb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4ef0a26f1324bd9a86b92d8346f4dd4",
              "IPY_MODEL_f816a3afc67c4db09024f7bdb7ff59dc",
              "IPY_MODEL_2738125907144cff9e228ccd6f6e4138"
            ],
            "layout": "IPY_MODEL_0504a26627304d1b929dd3cba0f46384"
          }
        },
        "8ae8291bff194e57aafa8120bd2bdb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f154b8c1fc74bcfa75b7652537257ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73213189a25c4db4943b95f5e8ceb7d6",
            "placeholder": "",
            "style": "IPY_MODEL_29e8cd966e874a6bb2f5200d91fdb923",
            "value": " 9/9 [00:00&lt;00:00, 11.01it/s]"
          }
        },
        "8fb2a4fc36b14239971379c65232a73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92b6cd50272b4f11a4197e1a3381b6b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92e9f350977049698d4418d86e467522": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ef98528ae94479d8ac737ddb88bb988",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7aa88d56705a4c83a61cd5f5056e2ab0",
            "value": 9
          }
        },
        "9b693b0b17214bb087e78d6a0fdc9a21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4a1f5cf2b004536bfcfac2c044b66cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ce6aebb2e64bcc9908027e4390b279": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c45a6eaa644f6a9163e27940ad84eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db83947234ab43f98913f3bd77cf65a8",
              "IPY_MODEL_92e9f350977049698d4418d86e467522",
              "IPY_MODEL_8f154b8c1fc74bcfa75b7652537257ca"
            ],
            "layout": "IPY_MODEL_59705729645744129ea854f5c529f747"
          }
        },
        "d7fd83c4f42f41be9636356b6e57403e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44c2240be0644591aaae680e02388df0",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26f8ba678bd94de790854d597858fa89",
            "value": 170498071
          }
        },
        "db83947234ab43f98913f3bd77cf65a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_515ed075a5b24d40a6892fb7d2d278d9",
            "placeholder": "",
            "style": "IPY_MODEL_8ae8291bff194e57aafa8120bd2bdb4c",
            "value": "CLEVER untargeted: 100%"
          }
        },
        "e4ef0a26f1324bd9a86b92d8346f4dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5124a2c9c46948f487c4b9d0ba6b6ac7",
            "placeholder": "",
            "style": "IPY_MODEL_92b6cd50272b4f11a4197e1a3381b6b4",
            "value": "CLEVER untargeted: 100%"
          }
        },
        "ee4e4045a51649baae806d2dcc3f88a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3ce6aebb2e64bcc9908027e4390b279",
            "placeholder": "",
            "style": "IPY_MODEL_8fb2a4fc36b14239971379c65232a73d",
            "value": " 170498071/170498071 [00:01&lt;00:00, 116470547.20it/s]"
          }
        },
        "f816a3afc67c4db09024f7bdb7ff59dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b693b0b17214bb087e78d6a0fdc9a21",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86095b75c15a4e0cbc033194d1e8539e",
            "value": 9
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
